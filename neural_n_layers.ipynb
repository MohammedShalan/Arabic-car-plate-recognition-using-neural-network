{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import skimage.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork :\n",
    "    # init the nerual network\n",
    "    def initialize_parameters(self,layerdims):\n",
    "        parameters={}\n",
    "        L = len(layerdims)\n",
    "        for i in range(1, L):\n",
    "            #parameters[\"w\"+str(i)] = np.random.randn( layerdims[i], layerdims[i-1]) * 0.1\n",
    "            # normal distribution \n",
    "            parameters[\"w\"+str(i)] = np.random.normal(0.0, pow(layerdims[i-1], -0.5), ( layerdims[i], layerdims[i-1]))\n",
    "\n",
    "            #parameters[\"b\"+str(i)] = np.zeros( (layerdims[i],1))\n",
    "\n",
    "        return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetwork = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def feedforward(Input,Weights,activation):\n",
    "    \n",
    "    #Z = np.add( np.dot(Weights,prev), bias)\n",
    "    Z = np.dot(Weights,Input)\n",
    "    \n",
    "     #Activation Function \n",
    "    if activation== \"sigmoid\":\n",
    "        A=1/(1+np.exp(-Z))\n",
    "        \n",
    "    layerParam =(Input,Weights,Z)\n",
    "    return A,layerParam\n",
    "\n",
    "def LayersForward(_input,_size,_params):\n",
    "    #print(\"input shape: {}\".format(_input.shape))\n",
    "    prev = _input\n",
    "    layerParams = []\n",
    "    for i in range(1,len(_size)):\n",
    "        prev,layerParam = feedforward(prev,_params[\"w\"+str(i)],\"sigmoid\")\n",
    "        layerParams.append(layerParam)\n",
    "        \n",
    "    return prev,layerParams\n",
    "\n",
    "def calculateCost(predict,target,function=\"Quadratic\"):\n",
    "    \n",
    "    if function== \"Quadratic\":\n",
    "        #cost = .5 * np.sum((target-predict)**2)\n",
    "        \n",
    "        # cost function = 1/2n |target-predict|^2 \n",
    "        # for single training set\n",
    "        cost = .5 * np.sum( (np.abs(target-predict))**2 )\n",
    "        return cost\n",
    "\n",
    "\n",
    "def feedbackward(dError,layerParam,activation=\"sigmoid\"):\n",
    "    \n",
    "    prev,Weights,Z = layerParam\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        dZ = dError * s * (1-s)\n",
    "        \n",
    "    # Computing derivative of Cost wrt A & W  & b\n",
    "    dError_prev = np.dot(Weights.transpose(), dZ)\n",
    "    #print(dError_prev.shape)\n",
    "    dW = 1/len(trainX) * np.dot(dZ, dError_prev.transpose())\n",
    "    #print(dW.shape)\n",
    "\n",
    "    #db =  np.sum(dZ, axis=1, keepdims=True)\n",
    "   \n",
    "    \n",
    "    return dError_prev, dW\n",
    "\n",
    "def layersBackward(predicted, target , layerParams, networkSize):\n",
    "    \n",
    "    currentOutput = predicted\n",
    "    L = len(networkSize) -1\n",
    "    grads={}\n",
    "    \n",
    "\n",
    "    # derivation of Cost wrt current Output\n",
    "    deriv_error = 2 * (currentOutput - target)\n",
    "    \n",
    "    #print(\"deriv_error shape: {}\".format(deriv_error.flatten().shape))\n",
    "    grads[\"dA\"+str(L)], grads[\"dW\"+str(L)] = feedbackward(deriv_error, layerParams[-1], \"sigmoid\")\n",
    "    \n",
    "    for i in list(reversed(range(L-1))):      \n",
    "        #print(\"backdrop in layer {}\".format(i))\n",
    "        current_Layerparams = layerParams[i]\n",
    "        a,b = feedbackward(grads[\"dA\"+str(i+2)], current_Layerparams, activation=\"sigmoid\")\n",
    "        grads[\"dA\"+str(i+1)] = a\n",
    "        #print(grads[\"dA\"+str(i+1)])\n",
    "        grads[\"dW\"+str(i+1)] = b\n",
    "        #grads[\"db\"+str(i+1)] = c\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    # each layer has 2 parameters (W,b)\n",
    "    L = len(params) // 2\n",
    "    for l in range(L):\n",
    "        params[\"w\"+str(l+1)] = params[\"w\"+str(l+1)] - learning_rate * grads[\"dW\"+str(l+1)]\n",
    "        #params[\"b\"+str(l+1)] = params[\"b\"+str(l+1)] - learning_rate * grads[\"db\"+str(l+1)]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: 21\n",
      "Total Images: 383\n",
      "Test images: 193 (33%)\n"
     ]
    }
   ],
   "source": [
    "cwd=os.getcwd()\n",
    "data_path_training = cwd + '/training'\n",
    "data_path_testing = cwd + '/testing'\n",
    "\n",
    "directories = [d for d in os.listdir(data_path_training) \n",
    "                   if os.path.isdir(os.path.join(data_path_training, d))]\n",
    "test_images = []\n",
    "test_labels = []\n",
    "labels = []\n",
    "images = []\n",
    "for image in os.listdir(data_path_testing):\n",
    "    test_labels.append(image.split(\"_\")[0])\n",
    "    url = os.path.join(data_path_testing,image)\n",
    "    img = np.array(Image.open(url))\n",
    "    test_images.append(img)\n",
    "for d in directories:\n",
    "    label_dir = os.path.join(data_path_training, d)\n",
    "    filenames = [os.path.join(label_dir, file) for file in os.listdir(label_dir) if file.endswith(('.png','.jpg')) ]\n",
    "    \n",
    "    for f in filenames:\n",
    "        img = np.array(Image.open(f))\n",
    "        images.append(img)\n",
    "        labels.append(d)\n",
    "\n",
    "print(\"Unique Classes: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n",
    "test_percentage = len(test_images) / (len(images) + len(test_images)) \n",
    "print(\"Test images: {} ({}%)\".format(len(test_images),int(test_percentage*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "for record in images:\n",
    "        # scale and shift the inputs\n",
    "        Input = ((np.asfarray(record) / 255.0 * 0.99) + 0.01 ).flatten()\n",
    "        Input = np.array(Input, ndmin=2).T\n",
    "        trainX.append(Input)\n",
    "for label in labels:\n",
    "        trainY.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainModel(trainX,trainY,iterations,Params,learningRate):\n",
    "    costs=[]  \n",
    "    # Gradient Descent\n",
    "    print(\"in progress..... \\n\")\n",
    "    for i in range(iterations):\n",
    "        label_index = 0\n",
    "        progress = \"==\"\n",
    "        for record in trainX:            \n",
    "            # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "            targets = np.zeros(output_nodes) + 0.01\n",
    "            # the target label for this record\n",
    "            target_class = trainY[label_index]    \n",
    "            # feedforward propagation\n",
    "            pred_output, layerHistory = LayersForward(record, layerSize,Params)\n",
    "            # scale and shift the desired outputs \n",
    "            targets[int(target_class)] = 0.99\n",
    "            # convert targets array to fit 2-dims as inputs array\n",
    "            targets = np.array(targets, ndmin=2).T\n",
    "            # Compute Cost  \n",
    "            cost = calculateCost(pred_output, targets)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = layersBackward(pred_output, targets, layerHistory, layerSize)\n",
    "\n",
    "            # Update Parameters\n",
    "            Params = update_params(Params, grads, learningRate)\n",
    "            label_index +=1\n",
    "        progress = progress + \"==\"\n",
    "        # + \"{}%\".format((i/epochs)*100)\n",
    "        print(progress,end=\"\", flush=True)    \n",
    "        if i%200 == 0:\n",
    "            print(f\"cost {i}: {np.squeeze(cost)}\")\n",
    "        if i%100 ==0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    \n",
    "    return parameters,costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_nodes = 6720\n",
    "hidden1_nodes = 300\n",
    "#hidden2_nodes = 25\n",
    "output_nodes = 22\n",
    "\n",
    "epochs = 2000\n",
    "learning_rate = .005\n",
    "\n",
    "layerSize = [input_nodes,hidden1_nodes,output_nodes]\n",
    "\n",
    "_params = NeuralNetwork.initialize_parameters(layerSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params,costs = trainModel(trainX,trainY,epochs,_params,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1': array([[ 2.55141886e-03, -7.87950083e-05, -6.40655996e-03, ...,\n",
      "         1.85785482e-02, -8.72739677e-04,  1.37840423e-02],\n",
      "       [-5.82781858e-03, -1.55613282e-02,  1.21875721e-03, ...,\n",
      "         7.12524592e-04, -1.69519687e-02, -1.44990992e-03],\n",
      "       [-1.28570721e-02, -1.77369827e-02, -1.40797103e-02, ...,\n",
      "         4.35023651e-03,  8.13778728e-03,  1.28723121e-03],\n",
      "       ...,\n",
      "       [ 2.63724924e-02,  5.33093870e-03,  3.58681868e-04, ...,\n",
      "        -1.28630176e-03, -1.25017775e-02,  1.15032441e-02],\n",
      "       [-1.61071931e-02, -3.18199096e-03,  1.70274871e-03, ...,\n",
      "        -3.47196334e-05, -1.57078152e-03, -9.19750924e-03],\n",
      "       [-2.42271801e-02,  4.17678084e-03, -1.20706667e-02, ...,\n",
      "        -2.25607115e-02, -8.02588787e-03, -4.20555707e-03]]), 'w2': array([[-0.12113286, -0.01780819, -0.00147444, ..., -0.02229468,\n",
      "        -0.01507767, -0.0295221 ],\n",
      "       [ 0.04487726,  0.06756397, -0.01704322, ...,  0.11430683,\n",
      "         0.0411135 , -0.00671927],\n",
      "       [-0.07174389, -0.00056904, -0.03250626, ...,  0.02095756,\n",
      "        -0.02421375, -0.04888878],\n",
      "       ...,\n",
      "       [-0.09263228,  0.01959345, -0.0843731 , ...,  0.01541843,\n",
      "        -0.05087144,  0.03045796],\n",
      "       [-0.01713797, -0.07429888, -0.13185335, ..., -0.02968188,\n",
      "        -0.02656453, -0.025615  ],\n",
      "       [-0.05428546,  0.081369  ,  0.0266816 , ...,  0.00604965,\n",
      "         0.05010603,  0.00145508]])}\n"
     ]
    }
   ],
   "source": [
    "print(_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1': array([[-0.00310369,  0.01557699,  0.00762785, ...,  0.00439039,\n",
      "         0.00860662,  0.02200343],\n",
      "       [ 0.00630686,  0.00617807, -0.00017239, ...,  0.00192625,\n",
      "        -0.01187784,  0.00607934],\n",
      "       [-0.00823879, -0.00194859,  0.02226215, ...,  0.0228755 ,\n",
      "         0.01100692, -0.01881993],\n",
      "       ...,\n",
      "       [ 0.00066678, -0.0025357 ,  0.02227682, ...,  0.0054767 ,\n",
      "        -0.00911768, -0.013234  ],\n",
      "       [ 0.00604884, -0.00365842,  0.00642758, ..., -0.00245439,\n",
      "        -0.01035516,  0.02190383],\n",
      "       [-0.01025729,  0.00189073,  0.01729239, ...,  0.02460347,\n",
      "         0.01481624,  0.00169448]]), 'w2': array([[ 0.0277558 ,  0.08807167,  0.10994625, ...,  0.09037308,\n",
      "        -0.06557057,  0.03791188],\n",
      "       [-0.06743832,  0.08148336,  0.0207109 , ...,  0.00686111,\n",
      "        -0.01120199,  0.07230443],\n",
      "       [-0.01199261, -0.05646797, -0.08772527, ...,  0.00215723,\n",
      "        -0.01589473, -0.00391438],\n",
      "       ...,\n",
      "       [ 0.09547844, -0.0162223 ,  0.06177145, ...,  0.01875059,\n",
      "         0.02879944, -0.09485313],\n",
      "       [ 0.05223963,  0.12281882,  0.16138453, ...,  0.01543715,\n",
      "         0.09156463, -0.02517739],\n",
      "       [-0.02033172,  0.11198421,  0.03886676, ..., -0.00841548,\n",
      "        -0.00983689,  0.0858579 ]])}\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAAD8CAYAAAAojwurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4oYwS9wQZokbwixxQ5gl/wXbf1mfQY3fJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output is 21\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0,len(images))\n",
    "display(index)\n",
    "\n",
    "pred_output, _ = LayersForward(trainX[index], layerSize,_params)\n",
    "print(\"output is %s\" % labels[index])\n",
    "#print(\"prediction is %s \"% np.argmax(pred_output))\n",
    "print(np.argmax(pred_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(index,Type=\"train\"):\n",
    "    if Type==\"train\":\n",
    "        plt.imshow(images[index])\n",
    "        \n",
    "    elif Type==\"test\":\n",
    "        plt.imshow(test_images[index])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00310538  0.0155775   0.00762999 ...  0.00438808  0.00860514\n",
      "   0.02200462]\n",
      " [ 0.00630438  0.00617866 -0.00016964 ...  0.00192349 -0.01187966\n",
      "   0.00608105]\n",
      " [-0.0082392  -0.00194852  0.02226273 ...  0.02287512  0.0110066\n",
      "  -0.01881978]\n",
      " ...\n",
      " [ 0.00066697 -0.00253571  0.02227658 ...  0.00547681 -0.00911749\n",
      "  -0.01323409]\n",
      " [ 0.00604876 -0.0036583   0.00642777 ... -0.00245468 -0.01035525\n",
      "   0.021904  ]\n",
      " [-0.01025817  0.00189085  0.01729331 ...  0.02460277  0.01481567\n",
      "   0.00169509]]\n",
      "------------------\n",
      "[[-5.25099399e-10  1.41284700e-10  1.01203615e-09 ... -1.26405826e-09\n",
      "  -7.30020740e-10  1.72524416e-10]\n",
      " [-1.89981736e-10  5.11170125e-11  3.66156171e-10 ... -4.57338140e-10\n",
      "  -2.64122579e-10  6.24195878e-11]\n",
      " [ 1.41109579e-11 -3.79673346e-12 -2.71963739e-11 ...  3.39689456e-11\n",
      "   1.96177942e-11 -4.63623608e-12]\n",
      " ...\n",
      " [ 2.76117784e-11 -7.42930163e-12 -5.32168157e-11 ...  6.64691229e-11\n",
      "   3.83873433e-11 -9.07200798e-12]\n",
      " [-8.04564761e-11  2.16478425e-11  1.55065617e-10 ... -1.93680803e-10\n",
      "  -1.11854815e-10  2.64344362e-11]\n",
      " [ 5.56432387e-11 -1.49715241e-11 -1.07242494e-10 ...  1.33948535e-10\n",
      "   7.73581505e-11 -1.82819049e-11]]\n"
     ]
    }
   ],
   "source": [
    "label_index = 16\n",
    "# create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "targets = np.zeros(output_nodes) + 0.01\n",
    "# the target label for this record\n",
    "target_class = trainY[label_index]    \n",
    "# feedforward propagation\n",
    "pred_output, layerHistory = LayersForward(trainX[label_index], layerSize,_params)\n",
    "# scale and shift the desired outputs \n",
    "targets[int(target_class)] = 0.99\n",
    "# convert targets array to fit 2-dims as inputs array\n",
    "targets = np.array(targets, ndmin=2).T\n",
    "# Compute Cost  \n",
    "cost = calculateCost(pred_output, targets)\n",
    "\n",
    "# Backward propagation\n",
    "grads = layersBackward(pred_output, targets, layerHistory, layerSize)\n",
    "_params[\"w1\"] = _params[\"w1\"] - learning_rate * grads[\"dW1\"]\n",
    "_params[\"w2\"] = _params[\"w2\"] - learning_rate * grads[\"dW2\"]\n",
    "\n",
    "# Update Parameters\n",
    "#_params = update_params(_params, grads, learning_rate)\n",
    "print(_params[\"w1\"] - learning_rate * grads[\"dW1\"])\n",
    "print(\"------------------\")\n",
    "\n",
    "print(learning_rate * grads[\"dW1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
